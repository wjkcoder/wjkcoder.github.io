{"posts":[{"title":"SpringBoot Run","content":" 创建SpringApplication SpringApplication#run SpringApplicationRunListeners实例 SpringApplicationRunListeners实例开启监听 prepare环境变量 创建ApplicationContext 创建注解beanDefinitionReader 创建ClassPathBeanDefinitionScanner 注册默认filter SpringApplication#prepareContext AbstractApplicationContext#refresh AbstractApplicationContext#prepareRefresh AbstractApplicationContext#prepareBeanFactory 添加BPP ApplicationContextAwareProcessor 添加ignore接口 注册dependency 添加BPP ApplicationListenerDetector 注册默认环境bean到singleton postProcessBeanFactory invokeBeanFactoryPostProcessors invokeBeanDefinitionRegistryPostProcessors ConfigurationClassPostProcessor#processConfigBeanDefinitions ConfigurationClassParser#parse invokeBeanFactoryPostProcessors 分批次执行BeanFactoryProcessor registerBeanPostProcessors initMessageSource initApplicationEventMulticaster onfresh registerListeners finishBeanFactoryInitialization 初始化bean 1.判断不是abstract，是单例非懒加载，执行getBean 2.AbstractBeanFactory#doGetbean 执行bean后置处理 finishRefresh SpringApplication#afterRefresh listeners响应执行start事件 SpringBoot的启动过程 simple @SpringBootApplication public class Application { public static void main(String[] args) { ConfigurableApplicationContext ctx = SpringApplication.run(Application.class); UserService userService = ctx.getBean(UserService.class); userService.setUsername(&quot;wonder&quot;); System.out.println(userService.getUsername()); } } 创建SpringApplication 推测webApplicationType 根据Spring factories加载接口实现类名称 BootstrapRegistryInitializer ApplicationContextInitializer ApplicationListener public SpringApplication(ResourceLoader resourceLoader, Class&lt;?&gt;... primarySources) { this.resourceLoader = resourceLoader; Assert.notNull(primarySources, &quot;PrimarySources must not be null&quot;); this.primarySources = new LinkedHashSet&lt;&gt;(Arrays.asList(primarySources)); this.webApplicationType = WebApplicationType.deduceFromClasspath(); this.bootstrapRegistryInitializers = new ArrayList&lt;&gt;( getSpringFactoriesInstances(BootstrapRegistryInitializer.class)); setInitializers((Collection) getSpringFactoriesInstances(ApplicationContextInitializer.class)); setListeners((Collection) getSpringFactoriesInstances(ApplicationListener.class)); this.mainApplicationClass = deduceMainApplicationClass(); } SpringApplication#run public ConfigurableApplicationContext run(String... args) { long startTime = System.nanoTime(); DefaultBootstrapContext bootstrapContext = createBootstrapContext(); ConfigurableApplicationContext context = null; configureHeadlessProperty(); SpringApplicationRunListeners listeners = getRunListeners(args); listeners.starting(bootstrapContext, this.mainApplicationClass); try { ApplicationArguments applicationArguments = new DefaultApplicationArguments(args); ConfigurableEnvironment environment = prepareEnvironment(listeners, bootstrapContext, applicationArguments); configureIgnoreBeanInfo(environment); Banner printedBanner = printBanner(environment); context = createApplicationContext(); context.setApplicationStartup(this.applicationStartup); prepareContext(bootstrapContext, context, environment, listeners, applicationArguments, printedBanner); refreshContext(context); afterRefresh(context, applicationArguments); Duration timeTakenToStartup = Duration.ofNanos(System.nanoTime() - startTime); if (this.logStartupInfo) { new StartupInfoLogger(this.mainApplicationClass).logStarted(getApplicationLog(), timeTakenToStartup); } listeners.started(context, timeTakenToStartup); callRunners(context, applicationArguments); } catch (Throwable ex) { handleRunFailure(context, ex, listeners); throw new IllegalStateException(ex); } try { Duration timeTakenToReady = Duration.ofNanos(System.nanoTime() - startTime); listeners.ready(context, timeTakenToReady); } catch (Throwable ex) { handleRunFailure(context, ex, null); throw new IllegalStateException(ex); } return context; } SpringApplicationRunListeners实例 根据Spring factories加载SpringApplicationRunListener实现类名称，创建SpringApplicationRunListeners实例 SpringApplicationRunListeners实例开启监听 prepare环境变量 创建ApplicationContext 根据webApplicationType创建ApplicationContext ApplicationContextFactory DEFAULT = (webApplicationType) -&gt; { try { switch (webApplicationType) { case SERVLET: return new AnnotationConfigServletWebServerApplicationContext(); case REACTIVE: return new AnnotationConfigReactiveWebServerApplicationContext(); default: return new AnnotationConfigApplicationContext(); } } catch (Exception ex) { throw new IllegalStateException(&quot;Unable create a default ApplicationContext instance, &quot; + &quot;you may need a custom ApplicationContextFactory&quot;, ex); } }; 这里若不是web项目，则创建AnnotationConfigApplicationContext public AnnotationConfigApplicationContext() { StartupStep createAnnotatedBeanDefReader = this.getApplicationStartup().start(&quot;spring.context.annotated-bean-reader.create&quot;); this.reader = new AnnotatedBeanDefinitionReader(this); createAnnotatedBeanDefReader.end(); this.scanner = new ClassPathBeanDefinitionScanner(this); } 创建注解beanDefinitionReader 根据实际情况选择添加以下beanDefinition到bdMap中 org.springframework.context.annotation.internalConfigurationAnnotationProcessor org.springframework.context.annotation.internalAutowiredAnnotationProcessor org.springframework.context.annotation.internalCommonAnnotationProcessor org.springframework.context.annotation.internalPersistenceAnnotationProcessor org.springframework.context.event.internalEventListenerProcessor org.springframework.context.event.internalEventListenerFactory 创建ClassPathBeanDefinitionScanner 注册默认filter Component.class JSR250 javax.annotation.ManagedBean JSR330 javax.inject.Named SpringApplication#prepareContext 1.添加一个close事件监听 AbstractApplicationContext#refresh AbstractApplicationContext#prepareRefresh initPropertySources env验证必要属性 AbstractApplicationContext#prepareBeanFactory 添加BPP ApplicationContextAwareProcessor 添加ignore接口 EnvironmentAware EmbeddedValueResolverAware ResourceLoaderAware ApplicationEventPublisherAware MessageSourceAware ApplicationContextAware ApplicationStartupAware 注册dependency 添加BPP ApplicationListenerDetector 注册默认环境bean到singleton environment systemProperties systemEnvironment applicationStartup postProcessBeanFactory invokeBeanFactoryPostProcessors invokeBeanDefinitionRegistryPostProcessors ConfigurationClassPostProcessor#processConfigBeanDefinitions ConfigurationClassParser#parse 判断是否有@Component，若有则处理成员变量 处理任何有@PropertySource 处理@ComponentScan 1.1 处理basePackages 处理basePackageClasses 若最终basePackages为空，添加配置类所在package到basePackages 执行scanner.doScan 遍历每一个basePackage，组装路径，拿到所有class文件 遍历每一个class，拿到注解等元数据，判断是否满足扫描标准 若满足，生成一个bd ScannedGenericBeanDefinition 判断bd是否满足添加到bdSet的需求，若满足，添加并注册到bdSet 遍历bdSet，bd包装成BeanDefinitionHolder并注册 处理@Import 处理@ImportResource 处理@Bean 处理interface default method 处理父类 invokeBeanFactoryPostProcessors 分批次执行BeanFactoryProcessor PriorityOrdered Ordered all other BeanFactoryPostProcessors registerBeanPostProcessors 注册实现PriorityOrdered接口的 注册实现Ordered接口的 注册所有的 regular BeanPostProcessors 重新注册internal BeanPostProcessors 添加BPP ApplicationListenerDetector initMessageSource 注册messageSource到singleton initApplicationEventMulticaster 实例化一个SimpleApplicationEventMulticaster并注册到singleton onfresh 子类的实现，初始化某些特殊的bean，比如ServletWebServerApplicationContext就会再次创建webServer registerListeners 所有的ApplicationListener绑定到eventMulticaster 添加ApplicationListenerBean 广播事件 finishBeanFactoryInitialization 初始化bean 1.判断不是abstract，是单例非懒加载，执行getBean 2.AbstractBeanFactory#doGetbean getSingleton获取到实例，直接返回 标记为已创建 处理依赖，若依赖没有被实例化，执行getBean（dep） getSingleton（beanName，beanFactory） 获取object，若空，执行beanFactory的getObject方法 调用getObject中的createBean resolveBeforeInstantiation 若需要代理，相关beanPostProcessor在此创建后返回 doCreateBean 移除缓存 允许MergedBeanDefinitionPostProcessor修改beanDefinition 提前暴露 populateBean initializeBean 执行aware方法 BeanNameAware BeanClassLoaderAware BeanFactoryAware 执行beanPostProcessor的before 执行initMethod 执行beanPostProcessor的after 暴露并注册 添加object到singleton 执行bean后置处理 finishRefresh clearResourceCaches，清除一些元数据缓存 initLifecycleProcessor，赋值lifeCycleProcessor 执行lifecycyleProcessor的refresh publishEvent，发布一个refreshed事件 SpringApplication#afterRefresh listeners响应执行start事件 ","link":"https://wjkcoder.github.io/post/springboot-run/"},{"title":"Netty Core Channel Read&Write","content":" NioSocketChannel read实现 AbstractNioByteChannel#read NioByteUnsafe#read NioSocketChannel#doReadBytes AbstractByteBuf#writeBytes PooledByteBuf#setBytes JDK SocketChannelImpl#read IOUtil#read IOUtil#readIntoNativeBuffer SocketDispatcher#read FileDispatcherImpl#read0 JNI read0 io.h #read NioSocketChannel Write实现 AbstractChannelHandlerContext#writeAndFlush invokeWrite0 unsafe#write invokeFlush0 unsafe#flush flush0 NioSocketChannel#doWrite JDK SocketChannel#wirte IOUtil#write IOUtil#writeFromNativeBuffer SocketDispatcher#write FileDispatcher#write0 JNI write0 io.h #write 零拷贝是Netty自身实现的吗？ 至少在读/写操作时，零拷贝并不是Netty去实现的，而是jdk的NIO本身提供的api NioSocketChannel read实现 起点在NioEventLoop的run中的processSelectedKeys，调用unsafe的read AbstractNioByteChannel#read 这里和NSSC的AbstractNioMessageChannel区分开，这里的byte是专门处理字节流的, 具体参与处理的其内部实现类NioByteUnsafe NioByteUnsafe#read 1.分配一块ByteBuffer 2.把channel的字节流读到buffer 3.处理完之后的buffer作为参数处理之后的pipline.fireChannelRead这里跟io读写相关的只发生在2，fireChannelRead可以看作是业务层面的处理，因为底层的字节流已经拿到了。下面只讨论2 protected class NioByteUnsafe extends AbstractNioUnsafe { @Override public final void read() { final ChannelConfig config = config(); if (shouldBreakReadReady(config)) { clearReadPending(); return; } final ChannelPipeline pipeline = pipeline(); final ByteBufAllocator allocator = config.getAllocator(); final RecvByteBufAllocator.Handle allocHandle = recvBufAllocHandle(); allocHandle.reset(config); ByteBuf byteBuf = null; boolean close = false; try { do { byteBuf = allocHandle.allocate(allocator); allocHandle.lastBytesRead(doReadBytes(byteBuf)); if (allocHandle.lastBytesRead() &lt;= 0) { // nothing was read. release the buffer. byteBuf.release(); byteBuf = null; close = allocHandle.lastBytesRead() &lt; 0; if (close) { // There is nothing left to read as we received an EOF. readPending = false; } break; } allocHandle.incMessagesRead(1); readPending = false; pipeline.fireChannelRead(byteBuf); byteBuf = null; } while (allocHandle.continueReading()); allocHandle.readComplete(); pipeline.fireChannelReadComplete(); } } NioSocketChannel#doReadBytes protected int doReadBytes(ByteBuf byteBuf) throws Exception { final RecvByteBufAllocator.Handle allocHandle = unsafe().recvBufAllocHandle(); allocHandle.attemptedBytesRead(byteBuf.writableBytes()); return byteBuf.writeBytes(javaChannel(), allocHandle.attemptedBytesRead()); } AbstractByteBuf#writeBytes public int writeBytes(ScatteringByteChannel in, int length) throws IOException { ensureWritable(length); int writtenBytes = setBytes(writerIndex, in, length); if (writtenBytes &gt; 0) { writerIndex += writtenBytes; } return writtenBytes; } PooledByteBuf#setBytes public final int setBytes(int index, ScatteringByteChannel in, int length) throws IOException { try { return in.read(internalNioBuffer(index, length)); } catch (ClosedChannelException ignored) { return -1; } } JDK SocketChannelImpl#read 核心部分 1.或者read同步锁 2.while执行IOUtil read，从fd读字节流到buffer public int read(ByteBuffer var1) throws IOException { if (var1 == null) { throw new NullPointerException(); } else { synchronized(this.readLock) { if (!this.ensureReadOpen()) { return -1; } else { int var3 = 0; boolean var20 = false; byte var10000; byte var5; label360: { int var4; try { var20 = true; this.begin(); synchronized(this.stateLock) { if (!this.isOpen()) { var5 = 0; var20 = false; break label360; } this.readerThread = NativeThread.current(); } while(true) { var3 = IOUtil.read(this.fd, var1, -1L, nd); if (var3 != -3 || !this.isOpen()) { var4 = IOStatus.normalize(var3); var20 = false; break; } } } } } IOUtil#read 1.区分buffer的类型，是否是直接内存/堆外内存 2.若是直接内存，直接执行堆外内存的逻辑； 3.若是heap，需要先将heap转化为堆外内存，再执行2的逻辑 之所以认为不是Netty在读写时实现了零拷贝技术，是因为jdk本身就提供了堆内存和直接内存的实现，Netty只是在此基础上默认使用了DirectBuffer实例而已， 若自己用NIO的api来写read操作，在指定buffer的时候使用堆外内存也可以达到相同的效果； static int read(FileDescriptor var0, ByteBuffer var1, long var2, NativeDispatcher var4) throws IOException { if (var1.isReadOnly()) { throw new IllegalArgumentException(&quot;Read-only buffer&quot;); } else if (var1 instanceof DirectBuffer) { return readIntoNativeBuffer(var0, var1, var2, var4); } else { //这里要去分配一块直接内存 ByteBuffer var5 = Util.getTemporaryDirectBuffer(var1.remaining()); int var7; try { int var6 = readIntoNativeBuffer(var0, var5, var2, var4); var5.flip(); if (var6 &gt; 0) { var1.put(var5); } var7 = var6; } finally { Util.offerFirstTemporaryDirectBuffer(var5); } return var7; } } IOUtil#readIntoNativeBuffer 计算剩余要读的字节，使用dispatcher去读取 private static int readIntoNativeBuffer(FileDescriptor var0, ByteBuffer var1, long var2, NativeDispatcher var4) throws IOException { int var5 = var1.position(); int var6 = var1.limit(); assert var5 &lt;= var6; int var7 = var5 &lt;= var6 ? var6 - var5 : 0; if (var7 == 0) { return 0; } else { boolean var8 = false; int var9; if (var2 != -1L) { var9 = var4.pread(var0, ((DirectBuffer)var1).address() + (long)var5, var7, var2); } else { var9 = var4.read(var0, ((DirectBuffer)var1).address() + (long)var5, var7); } if (var9 &gt; 0) { var1.position(var5 + var9); } return var9; } } SocketDispatcher#read FileDispatcherImpl#read0 FileDispatcherImpl#read0 JNI read0 JNIEXPORT jint JNICALL Java_sun_nio_ch_FileDispatcherImpl_read0(JNIEnv *env, jclass clazz, jobject fdo, jlong address, jint len) { jint fd = fdval(env, fdo); void *buf = (void *)jlong_to_ptr(address); return convertReturnVal(env, read(fd, buf, len), JNI_TRUE); } io.h #read 从fd文件描述符所在文件，读size个字节到buf所在的内存地址 static inline int read(int fd, void* buf, unsigned int size) { return _read(fd, buf, size); } NioSocketChannel Write实现 写的起点是ctx.writeAndFlush AbstractChannelHandlerContext#writeAndFlush public ChannelFuture writeAndFlush(Object msg) { return writeAndFlush(msg, newPromise()); } public ChannelFuture writeAndFlush(Object msg, ChannelPromise promise) { write(msg, true, promise); return promise; } private void write(Object msg, boolean flush, ChannelPromise promise) { ObjectUtil.checkNotNull(msg, &quot;msg&quot;); try { if (isNotValidPromise(promise, true)) { ReferenceCountUtil.release(msg); // cancelled return; } } catch (RuntimeException e) { ReferenceCountUtil.release(msg); throw e; } final AbstractChannelHandlerContext next = findContextOutbound(flush ? (MASK_WRITE | MASK_FLUSH) : MASK_WRITE); final Object m = pipeline.touch(msg, next); EventExecutor executor = next.executor(); if (executor.inEventLoop()) { if (flush) { next.invokeWriteAndFlush(m, promise); } else { next.invokeWrite(m, promise); } } else { final WriteTask task = WriteTask.newInstance(next, m, promise, flush); if (!safeExecute(executor, task, promise, m, !flush)) { // We failed to submit the WriteTask. We need to cancel it so we decrement the pending bytes // and put it back in the Recycler for re-use later. // // See https://github.com/netty/netty/issues/8343. task.cancel(); } } } void invokeWriteAndFlush(Object msg, ChannelPromise promise) { if (invokeHandler()) { invokeWrite0(msg, promise); invokeFlush0(); } else { writeAndFlush(msg, promise); } } channel的字节流不是立刻发送的，分成了两个动作，一个缓存和组合message，另一个flush才是真正的发送； invokeWrite0 unsafe#write 1.有一个outboundBuffer，代表最后真正要发送的字节 2.过滤msg，通过之后的msg才会被加入到out中 3.添加msg public final void write(Object msg, ChannelPromise promise) { assertEventLoop(); ChannelOutboundBuffer outboundBuffer = this.outboundBuffer; if (outboundBuffer == null) { try { // release message now to prevent resource-leak ReferenceCountUtil.release(msg); } finally { // If the outboundBuffer is null we know the channel was closed and so // need to fail the future right away. If it is not null the handling of the rest // will be done in flush0() // See https://github.com/netty/netty/issues/2362 safeSetFailure(promise, newClosedChannelException(initialCloseCause, &quot;write(Object, ChannelPromise)&quot;)); } return; } int size; try { msg = filterOutboundMessage(msg); size = pipeline.estimatorHandle().size(msg); if (size &lt; 0) { size = 0; } } catch (Throwable t) { try { ReferenceCountUtil.release(msg); } finally { safeSetFailure(promise, t); } return; } outboundBuffer.addMessage(msg, size, promise); } invokeFlush0 unsafe#flush 1.拿到write之后的outboundBuffer 2.确认每一个promise 3.执行flush0 public final void flush() { assertEventLoop(); ChannelOutboundBuffer outboundBuffer = this.outboundBuffer; if (outboundBuffer == null) { return; } outboundBuffer.addFlush(); flush0(); } public void addFlush() { // There is no need to process all entries if there was already a flush before and no new messages // where added in the meantime. // // See https://github.com/netty/netty/issues/2577 Entry entry = unflushedEntry; if (entry != null) { if (flushedEntry == null) { // there is no flushedEntry yet, so start with the entry flushedEntry = entry; } do { flushed ++; if (!entry.promise.setUncancellable()) { // Was cancelled so make sure we free up memory and notify about the freed bytes int pending = entry.cancel(); decrementPendingOutboundBytes(pending, false, true); } entry = entry.next; } while (entry != null); // All flushed so reset unflushedEntry unflushedEntry = null; } } flush0 protected void flush0() { if (inFlush0) { // Avoid re-entrance return; } final ChannelOutboundBuffer outboundBuffer = this.outboundBuffer; if (outboundBuffer == null || outboundBuffer.isEmpty()) { return; } inFlush0 = true; // Mark all pending write requests as failure if the channel is inactive. if (!isActive()) { try { // Check if we need to generate the exception at all. if (!outboundBuffer.isEmpty()) { if (isOpen()) { outboundBuffer.failFlushed(new NotYetConnectedException(), true); } else { // Do not trigger channelWritabilityChanged because the channel is closed already. outboundBuffer.failFlushed(newClosedChannelException(initialCloseCause, &quot;flush0()&quot;), false); } } } finally { inFlush0 = false; } return; } try { doWrite(outboundBuffer); } catch (Throwable t) { handleWriteError(t); } finally { inFlush0 = false; } } NioSocketChannel#doWrite 1.获取nio的channel 2.获取要处理的buffer数组 3.执行channel.write protected void doWrite(ChannelOutboundBuffer in) throws Exception { SocketChannel ch = javaChannel(); int writeSpinCount = config().getWriteSpinCount(); do { if (in.isEmpty()) { // All written so clear OP_WRITE clearOpWrite(); // Directly return here so incompleteWrite(...) is not called. return; } // Ensure the pending writes are made of ByteBufs only. int maxBytesPerGatheringWrite = ((NioSocketChannelConfig) config).getMaxBytesPerGatheringWrite(); ByteBuffer[] nioBuffers = in.nioBuffers(1024, maxBytesPerGatheringWrite); int nioBufferCnt = in.nioBufferCount(); // Always use nioBuffers() to workaround data-corruption. // See https://github.com/netty/netty/issues/2761 switch (nioBufferCnt) { case 0: // We have something else beside ByteBuffers to write so fallback to normal writes. writeSpinCount -= doWrite0(in); break; case 1: { // Only one ByteBuf so use non-gathering write // Zero length buffers are not added to nioBuffers by ChannelOutboundBuffer, so there is no need // to check if the total size of all the buffers is non-zero. ByteBuffer buffer = nioBuffers[0]; int attemptedBytes = buffer.remaining(); final int localWrittenBytes = ch.write(buffer); if (localWrittenBytes &lt;= 0) { incompleteWrite(true); return; } adjustMaxBytesPerGatheringWrite(attemptedBytes, localWrittenBytes, maxBytesPerGatheringWrite); in.removeBytes(localWrittenBytes); --writeSpinCount; break; } default: { // Zero length buffers are not added to nioBuffers by ChannelOutboundBuffer, so there is no need // to check if the total size of all the buffers is non-zero. // We limit the max amount to int above so cast is safe long attemptedBytes = in.nioBufferSize(); final long localWrittenBytes = ch.write(nioBuffers, 0, nioBufferCnt); if (localWrittenBytes &lt;= 0) { incompleteWrite(true); return; } // Casting to int is safe because we limit the total amount of data in the nioBuffers to int above. adjustMaxBytesPerGatheringWrite((int) attemptedBytes, (int) localWrittenBytes, maxBytesPerGatheringWrite); in.removeBytes(localWrittenBytes); --writeSpinCount; break; } } } while (writeSpinCount &gt; 0); incompleteWrite(writeSpinCount &lt; 0); } JDK SocketChannel#wirte 1.获取wirte同步锁 2.执行IOUtil write public int write(ByteBuffer var1) throws IOException { if (var1 == null) { throw new NullPointerException(); } else { synchronized(this.writeLock) { this.ensureWriteOpen(); int var3 = 0; boolean var20 = false; byte var5; label310: { int var4; try { var20 = true; this.begin(); synchronized(this.stateLock) { if (!this.isOpen()) { var5 = 0; var20 = false; break label310; } this.writerThread = NativeThread.current(); } do { var3 = IOUtil.write(this.fd, var1, -1L, nd); } while(var3 == -3 &amp;&amp; this.isOpen()); var4 = IOStatus.normalize(var3); var20 = false; } finally { if (var20) { this.writerCleanup(); this.end(var3 &gt; 0 || var3 == -2); synchronized(this.stateLock) { if (var3 &lt;= 0 &amp;&amp; !this.isOutputOpen) { throw new AsynchronousCloseException(); } } assert IOStatus.check(var3); } } this.writerCleanup(); this.end(var3 &gt; 0 || var3 == -2); synchronized(this.stateLock) { if (var3 &lt;= 0 &amp;&amp; !this.isOutputOpen) { throw new AsynchronousCloseException(); } } assert IOStatus.check(var3); return var4; } this.writerCleanup(); this.end(var3 &gt; 0 || var3 == -2); synchronized(this.stateLock) { if (var3 &lt;= 0 &amp;&amp; !this.isOutputOpen) { throw new AsynchronousCloseException(); } } assert IOStatus.check(var3); return var5; } } } IOUtil#write 1.判断是否是堆外内存 2.若是堆外内存，直接执行内外内存处理逻辑； 3.不是堆外内存，创建一块堆外内存，把heapbuffer拷贝到堆外内存，再执行2的逻辑 static int write(FileDescriptor var0, ByteBuffer var1, long var2, NativeDispatcher var4) throws IOException { if (var1 instanceof DirectBuffer) { return writeFromNativeBuffer(var0, var1, var2, var4); } else { int var5 = var1.position(); int var6 = var1.limit(); assert var5 &lt;= var6; int var7 = var5 &lt;= var6 ? var6 - var5 : 0; ByteBuffer var8 = Util.getTemporaryDirectBuffer(var7); int var10; try { var8.put(var1);//heap拷贝到direct var8.flip();//切换读写模式 var1.position(var5); int var9 = writeFromNativeBuffer(var0, var8, var2, var4); if (var9 &gt; 0) { var1.position(var5 + var9); } var10 = var9; } finally { Util.offerFirstTemporaryDirectBuffer(var8); } return var10; } } IOUtil#writeFromNativeBuffer 1.确认要写的边界 2.执行dispatcher的write private static int writeFromNativeBuffer(FileDescriptor var0, ByteBuffer var1, long var2, NativeDispatcher var4) throws IOException { int var5 = var1.position(); int var6 = var1.limit(); assert var5 &lt;= var6; int var7 = var5 &lt;= var6 ? var6 - var5 : 0; boolean var8 = false; if (var7 == 0) { return 0; } else { int var9; if (var2 != -1L) { var9 = var4.pwrite(var0, ((DirectBuffer)var1).address() + (long)var5, var7, var2); } else { var9 = var4.write(var0, ((DirectBuffer)var1).address() + (long)var5, var7); } if (var9 &gt; 0) { var1.position(var5 + var9); } return var9; } } SocketDispatcher#write int write(FileDescriptor var1, long var2, int var4) throws IOException { return FileDispatcherImpl.write0(var1, var2, var4); } FileDispatcher#write0 static native int write0(FileDescriptor var0, long var1, int var3) throws IOException; JNI write0 1.获取文件描述符fd 2.获取要写入的buffer地址 JNIEXPORT jint JNICALL Java_sun_nio_ch_FileDispatcherImpl_write0(JNIEnv *env, jclass clazz, jobject fdo, jlong address, jint len) { jint fd = fdval(env, fdo); void *buf = (void *)jlong_to_ptr(address); return convertReturnVal(env, write(fd, buf, len), JNI_FALSE); } io.h #write 从buf往fd文件描述符打开的文件写入size个字节 static inline int write(int fd, const void* buf, unsigned int size) { return _write(fd, buf, size); } ","link":"https://wjkcoder.github.io/post/netty-core-channel-readandwrite/"},{"title":"Netty Core NioEventLoop","content":" NioEventLoop 启动 NioEventLoop register NioEventLoop execute SingleThreadEventExecutor #doStartThread NioEventLoop #run()处理事件和任务 Select()机制实现 NioEventLoop select NioEventLoop中wrappedSelector SelectorImpl#select KQueueSelectorImpl#doSelect KqueueWrapper #poll JNI kevent kevent.h KQueueSelectorImpl#updateSelectedKeys KQueueArrayWrapper.getDescriptor this.kqueueWrapper.getReventOps(var4) 更新SelectedKeys processSelectedKeys 处理响应事件 NioEventLoop#processSelectedKeys NioEventLoop#processSelectedKeysOptimized NioEventLoop#processSelectedKeys channel.unsafe().read() NioServerSocketChannel中unsafe实例#read NioServerSocketChannel#doReadMessages NioSocketChannel中unsafe实例#read pipline.fireChannelRead NioServerSocketChannel #fireChannelRead NioSocketChannel #fireChannelRead NioEventLoop 启动 NioEventLoop register NioEventLoop的实例化发生在NioEventLoopGroup的创建，但线程并没有开启，线程的开启是在register时发生的 1.将channel注册到eventloop上 2.判断eventloop是否是当前线程，非当前线程时，使用eventloop 执行task public final void register(EventLoop eventLoop, final ChannelPromise promise) { AbstractChannel.this.eventLoop = eventLoop; if (eventLoop.inEventLoop()) { register0(promise); } else { try { eventLoop.execute(new Runnable() { @Override public void run() { register0(promise); } }); } } } NioEventLoop execute 1.将task添加任务队列 2.判断eventloop是否开启，若未开启则执行startThread public void execute(Runnable task) { ObjectUtil.checkNotNull(task, &quot;task&quot;); execute(task, !(task instanceof LazyRunnable) &amp;&amp; wakesUpForTask(task)); } private void execute(Runnable task, boolean immediate) { boolean inEventLoop = inEventLoop(); addTask(task); if (!inEventLoop) { startThread(); } if (!addTaskWakesUp &amp;&amp; immediate) { wakeup(inEventLoop); } } private void startThread() { if (state == ST_NOT_STARTED) { if (STATE_UPDATER.compareAndSet(this, ST_NOT_STARTED, ST_STARTED)) { boolean success = false; try { doStartThread(); success = true; } finally { if (!success) { STATE_UPDATER.compareAndSet(this, ST_STARTED, ST_NOT_STARTED); } } } } } SingleThreadEventExecutor #doStartThread 1.执行包装好的runnable，thread赋值为当前线程也就是executor所在的线程 2.执行NioEventLoop的run，就是处理SelectedKeys的for循环 3.在外部加上shutdown和相关处理 private void doStartThread() { assert thread == null; executor.execute(new Runnable() { @Override public void run() { thread = Thread.currentThread(); if (interrupted) { thread.interrupt(); } boolean success = false; updateLastExecutionTime(); try { SingleThreadEventExecutor.this.run(); success = true; } catch (Throwable t) { logger.warn(&quot;Unexpected exception from an event executor: &quot;, t); } finally { for (;;) { int oldState = state; if (oldState &gt;= ST_SHUTTING_DOWN || STATE_UPDATER.compareAndSet( SingleThreadEventExecutor.this, oldState, ST_SHUTTING_DOWN)) { break; } } // Check if confirmShutdown() was called at the end of the loop. if (success &amp;&amp; gracefulShutdownStartTime == 0) { if (logger.isErrorEnabled()) { logger.error(&quot;Buggy &quot; + EventExecutor.class.getSimpleName() + &quot; implementation; &quot; + SingleThreadEventExecutor.class.getSimpleName() + &quot;.confirmShutdown() must &quot; + &quot;be called before run() implementation terminates.&quot;); } } try { // Run all remaining tasks and shutdown hooks. At this point the event loop // is in ST_SHUTTING_DOWN state still accepting tasks which is needed for // graceful shutdown with quietPeriod. for (;;) { if (confirmShutdown()) { break; } } // Now we want to make sure no more tasks can be added from this point. This is // achieved by switching the state. Any new tasks beyond this point will be rejected. for (;;) { int oldState = state; if (oldState &gt;= ST_SHUTDOWN || STATE_UPDATER.compareAndSet( SingleThreadEventExecutor.this, oldState, ST_SHUTDOWN)) { break; } } // We have the final set of tasks in the queue now, no more can be added, run all remaining. // No need to loop here, this is the final pass. confirmShutdown(); } finally { try { cleanup(); } finally { // Lets remove all FastThreadLocals for the Thread as we are about to terminate and notify // the future. The user may block on the future and once it unblocks the JVM may terminate // and start unloading classes. // See https://github.com/netty/netty/issues/6596. FastThreadLocal.removeAll(); STATE_UPDATER.set(SingleThreadEventExecutor.this, ST_TERMINATED); threadLock.countDown(); int numUserTasks = drainTasks(); if (numUserTasks &gt; 0 &amp;&amp; logger.isWarnEnabled()) { logger.warn(&quot;An event executor terminated with &quot; + &quot;non-empty task queue (&quot; + numUserTasks + ')'); } terminationFuture.setSuccess(null); } } } } }); } NioEventLoop #run()处理事件和任务 事件处理的核心 1.根据当前是否有task计算选择策略，tasks为空时，默认是SELECT 2.计算timeout，执行select()，这里若没有事件产生，阻塞当前线程 3.事件产生或超时，线程唤醒；根据事件数量判定是否执行processKeys 4.finally中处理tasks队列中的task protected void run() { int selectCnt = 0; for (;;) { try { int strategy; try { strategy = selectStrategy.calculateStrategy(selectNowSupplier, hasTasks()); switch (strategy) { case SelectStrategy.CONTINUE: continue; case SelectStrategy.BUSY_WAIT: // fall-through to SELECT since the busy-wait is not supported with NIO case SelectStrategy.SELECT: long curDeadlineNanos = nextScheduledTaskDeadlineNanos(); if (curDeadlineNanos == -1L) { curDeadlineNanos = NONE; // nothing on the calendar } nextWakeupNanos.set(curDeadlineNanos); try { if (!hasTasks()) { strategy = select(curDeadlineNanos); } } finally { // This update is just to help block unnecessary selector wakeups // so use of lazySet is ok (no race condition) nextWakeupNanos.lazySet(AWAKE); } // fall through default: } } catch (IOException e) { // If we receive an IOException here its because the Selector is messed up. Let's rebuild // the selector and retry. https://github.com/netty/netty/issues/8566 rebuildSelector0(); selectCnt = 0; handleLoopException(e); continue; } selectCnt++; cancelledKeys = 0; needsToSelectAgain = false; final int ioRatio = this.ioRatio; boolean ranTasks; if (ioRatio == 100) { try { if (strategy &gt; 0) { processSelectedKeys(); } } finally { // Ensure we always run tasks. ranTasks = runAllTasks(); } } else if (strategy &gt; 0) { final long ioStartTime = System.nanoTime(); try { processSelectedKeys(); } finally { // Ensure we always run tasks. final long ioTime = System.nanoTime() - ioStartTime; ranTasks = runAllTasks(ioTime * (100 - ioRatio) / ioRatio); } } else { ranTasks = runAllTasks(0); // This will run the minimum number of tasks } if (ranTasks || strategy &gt; 0) { if (selectCnt &gt; MIN_PREMATURE_SELECTOR_RETURNS &amp;&amp; logger.isDebugEnabled()) { logger.debug(&quot;Selector.select() returned prematurely {} times in a row for Selector {}.&quot;, selectCnt - 1, selector); } selectCnt = 0; } else if (unexpectedSelectorWakeup(selectCnt)) { // Unexpected wakeup (unusual case) selectCnt = 0; } } catch (CancelledKeyException e) { // Harmless exception - log anyway if (logger.isDebugEnabled()) { logger.debug(CancelledKeyException.class.getSimpleName() + &quot; raised by a Selector {} - JDK bug?&quot;, selector, e); } } catch (Error e) { throw (Error) e; } catch (Throwable t) { handleLoopException(t); } finally { // Always handle shutdown even if the loop processing threw an exception. try { if (isShuttingDown()) { closeAll(); if (confirmShutdown()) { return; } } } catch (Error e) { throw (Error) e; } catch (Throwable t) { handleLoopException(t); } } } } Select()机制实现 NioEventLoop select strategy = select(curDeadlineNanos); private int select(long deadlineNanos) throws IOException { if (deadlineNanos == NONE) { return selector.select(); } // Timeout will only be 0 if deadline is within 5 microsecs long timeoutMillis = deadlineToDelayNanos(deadlineNanos + 995000L) / 1000000L; return timeoutMillis &lt;= 0 ? selector.selectNow() : selector.select(timeoutMillis); } NioEventLoop中wrappedSelector 1.重置selectKeys 如果是用jdk的nio去做，也需要自己手动去重置selectKeys，否则会有错误的keys 2.执行unwrapped selector的select public int select() throws IOException { selectionKeys.reset(); return delegate.select(); } SelectorImpl#select public int select() throws IOException { return this.select(0L); } public int select(long var1) throws IOException { if (var1 &lt; 0L) { throw new IllegalArgumentException(&quot;Negative timeout&quot;); } else { return this.lockAndDoSelect(var1 == 0L ? -1L : var1); } } private int lockAndDoSelect(long var1) throws IOException { synchronized(this) { if (!this.isOpen()) { throw new ClosedSelectorException(); } else { int var10000; synchronized(this.publicKeys) { synchronized(this.publicSelectedKeys) { var10000 = this.doSelect(var1); } } return var10000; } } } KQueueSelectorImpl#doSelect 核心两件事情： 1.返回产生的事件数量 2.根据事件数量更新selectedKeys protected int doSelect(long var1) throws IOException { boolean var3 = false; if (this.closed) { throw new ClosedSelectorException(); } else { this.processDeregisterQueue(); int var7; try { this.begin(); var7 = this.kqueueWrapper.poll(var1); } finally { this.end(); } this.processDeregisterQueue(); return this.updateSelectedKeys(var7); } } KqueueWrapper #poll int poll(long var1) { this.updateRegistrations(); int var3 = this.kevent0(this.kq, this.keventArrayAddress, 128, var1); return var3; } JNI kevent JNIEXPORT jint JNICALL Java_sun_nio_ch_KQueueArrayWrapper_kevent0(JNIEnv *env, jobject this, jint kq, jlong kevAddr, jint kevCount, jlong timeout) { struct kevent *kevs = (struct kevent *)jlong_to_ptr(kevAddr); struct timespec ts; struct timespec *tsp; int result; // Java timeout is in milliseconds. Convert to struct timespec. // Java timeout == -1 : wait forever : timespec timeout of NULL // Java timeout == 0 : return immediately : timespec timeout of zero if (timeout &gt;= 0) { ts.tv_sec = timeout / 1000; ts.tv_nsec = (timeout % 1000) * 1000000; //nanosec = 1 million millisec tsp = &amp;ts; } else { tsp = NULL; } result = kevent(kq, NULL, 0, kevs, kevCount, tsp); if (result &lt; 0) { if (errno == EINTR) { // ignore EINTR, pretend nothing was selected result = 0; } else { JNU_ThrowIOExceptionWithLastError(env, &quot;KQueueArrayWrapper: kqueue failed&quot;); } } return result; } kevent.h 返回timeout内产生的事件个数，产生的event会记录在eventlist的连续地址内； int kevent(int kq, const struct kevent *changelist, int nchanges, struct kevent *eventlist, int nevents, const struct timespec *timeout); KQueueSelectorImpl#updateSelectedKeys 1.循环事件个数，以此为偏移量计算fd的identifier KQueueArrayWrapper.getDescriptor eventlist是连续的内存，一个kevent是32字节，fd identifier long 是在第一个，偏移量是0 int getDescriptor(int var1) { int var2 = SIZEOF_KEVENT * var1 + FD_OFFSET; if (is64bit) { long var3 = this.keventArray.getLong(var2); assert var3 &lt;= 2147483647L; return (int)var3; } else { return this.keventArray.getInt(var2); } } 2.根据偏移量访问直接内存，获取事件类型 this.kqueueWrapper.getReventOps(var4) 同样的，也是根据偏移量拿到index处的kevent，8字节(64bit)后是filter，getshort拿到filter int getReventOps(int var1) { int var2 = 0; int var3 = SIZEOF_KEVENT * var1 + FILTER_OFFSET; short var4 = this.keventArray.getShort(var3); if (var4 == EVFILT_READ) { var2 |= Net.POLLIN; } else if (var4 == EVFILT_WRITE) { var2 |= Net.POLLOUT; } return var2; } 更新SelectedKeys 这里的selectedkeys就是之后unwrapped selector中的keys private int updateSelectedKeys(int var1) throws IOException { int var2 = 0; boolean var3 = false; ++this.updateCount; for(int var4 = 0; var4 &lt; var1; ++var4) { int var5 = this.kqueueWrapper.getDescriptor(var4); if (var5 == this.fd0) { var3 = true; } else { MapEntry var6 = (MapEntry)this.fdMap.get(var5); if (var6 != null) { int var7 = this.kqueueWrapper.getReventOps(var4); SelectionKeyImpl var8 = var6.ski; if (this.selectedKeys.contains(var8)) { if (var6.updateCount != this.updateCount) { if (var8.channel.translateAndSetReadyOps(var7, var8)) { ++var2; var6.updateCount = this.updateCount; } } else { var8.channel.translateAndUpdateReadyOps(var7, var8); } } else { var8.channel.translateAndSetReadyOps(var7, var8); if ((var8.nioReadyOps() &amp; var8.nioInterestOps()) != 0) { this.selectedKeys.add(var8); ++var2; var6.updateCount = this.updateCount; } } } } } if (var3) { synchronized(this.interruptLock) { IOUtil.drain(this.fd0); this.interruptTriggered = false; } } return var2; } processSelectedKeys 处理响应事件 NioEventLoop#processSelectedKeys private void processSelectedKeys() { if (selectedKeys != null) { processSelectedKeysOptimized(); } else { processSelectedKeysPlain(selector.selectedKeys()); } } NioEventLoop#processSelectedKeysOptimized 从key的attach上拿到channel，执行处理key private void processSelectedKeysOptimized() { for (int i = 0; i &lt; selectedKeys.size; ++i) { final SelectionKey k = selectedKeys.keys[i]; // null out entry in the array to allow to have it GC'ed once the Channel close // See https://github.com/netty/netty/issues/2363 selectedKeys.keys[i] = null; final Object a = k.attachment(); if (a instanceof AbstractNioChannel) { processSelectedKey(k, (AbstractNioChannel) a); } else { @SuppressWarnings(&quot;unchecked&quot;) NioTask&lt;SelectableChannel&gt; task = (NioTask&lt;SelectableChannel&gt;) a; processSelectedKey(k, task); } } } NioEventLoop#processSelectedKeys 1.拿到channel的unsafe实例 2.判断k的事件类型，依次处理 OP_CONNECT &gt; OP_WRITE &gt; OP_READ | OP_ACCEPT,这里read和accept是放在一起处理的，原因是ServerChannel的accept操作也是在channelRead方法里执行的； 3.需要注意的是NioServerSocketChannel和NioSocketChannel的unsafe实例是不同的 前者对应AbstractNioMessageChannel的实现，用于处理连接； 后者对应AbstractNioByteChannel的实现，用于处理字节流读写； private void processSelectedKey(SelectionKey k, AbstractNioChannel ch) { final AbstractNioChannel.NioUnsafe unsafe = ch.unsafe(); try { int readyOps = k.readyOps(); // We first need to call finishConnect() before try to trigger a read(...) or write(...) as otherwise // the NIO JDK channel implementation may throw a NotYetConnectedException. if ((readyOps &amp; SelectionKey.OP_CONNECT) != 0) { // remove OP_CONNECT as otherwise Selector.select(..) will always return without blocking // See https://github.com/netty/netty/issues/924 int ops = k.interestOps(); ops &amp;= ~SelectionKey.OP_CONNECT; k.interestOps(ops); unsafe.finishConnect(); } // Process OP_WRITE first as we may be able to write some queued buffers and so free memory. if ((readyOps &amp; SelectionKey.OP_WRITE) != 0) { // Call forceFlush which will also take care of clear the OP_WRITE once there is nothing left to write ch.unsafe().forceFlush(); } // Also check for readOps of 0 to workaround possible JDK bug which may otherwise lead // to a spin loop if ((readyOps &amp; (SelectionKey.OP_READ | SelectionKey.OP_ACCEPT)) != 0 || readyOps == 0) { unsafe.read(); } } catch (CancelledKeyException ignored) { unsafe.close(unsafe.voidPromise()); } } channel.unsafe().read() NioServerSocketChannel中unsafe实例#read 1.调用NioServerSocketChannel的doReadMessages处理accept，把nio SocketChannel包装成NioSocketChannel放在readBuf列表中 2.执行pipline的channelRead方法，这里的实现是固定的，用于配置childHandler 3.执行pipline的channelReadComplete 4.处理异常(若有) private final class NioMessageUnsafe extends AbstractNioUnsafe { private final List&lt;Object&gt; readBuf = new ArrayList&lt;Object&gt;(); @Override public void read() { assert eventLoop().inEventLoop(); final ChannelConfig config = config(); final ChannelPipeline pipeline = pipeline(); final RecvByteBufAllocator.Handle allocHandle = unsafe().recvBufAllocHandle(); allocHandle.reset(config); boolean closed = false; Throwable exception = null; try { try { do { int localRead = doReadMessages(readBuf); if (localRead == 0) { break; } if (localRead &lt; 0) { closed = true; break; } allocHandle.incMessagesRead(localRead); } while (allocHandle.continueReading()); } catch (Throwable t) { exception = t; } int size = readBuf.size(); for (int i = 0; i &lt; size; i ++) { readPending = false; pipeline.fireChannelRead(readBuf.get(i)); } readBuf.clear(); allocHandle.readComplete(); pipeline.fireChannelReadComplete(); if (exception != null) { closed = closeOnReadError(exception); pipeline.fireExceptionCaught(exception); } if (closed) { inputShutdown = true; if (isOpen()) { close(voidPromise()); } } } finally { // Check if there is a readPending which was not processed yet. // This could be for two reasons: // * The user called Channel.read() or ChannelHandlerContext.read() in channelRead(...) method // * The user called Channel.read() or ChannelHandlerContext.read() in channelReadComplete(...) method // // See https://github.com/netty/netty/issues/2254 if (!readPending &amp;&amp; !config.isAutoRead()) { removeReadOp(); } } } } NioServerSocketChannel#doReadMessages 利用nio的api accept创建SocketChannel，包装成为NioSocketChannel protected int doReadMessages(List&lt;Object&gt; buf) throws Exception { SocketChannel ch = SocketUtils.accept(javaChannel()); try { if (ch != null) { buf.add(new NioSocketChannel(this, ch)); return 1; } } catch (Throwable t) { logger.warn(&quot;Failed to create a new channel from an accepted socket.&quot;, t); try { ch.close(); } catch (Throwable t2) { logger.warn(&quot;Failed to close a socket.&quot;, t2); } } return 0; } NioSocketChannel中unsafe实例#read 1.创建DirectBuffer，从channel中读取数据到buffer，作为参数执行pipline的fireChannelRead，netty的网络io层面的read操作已经完成了，handler中的操作属于是应用层自定义操作； 2.read完之后，执行pipline的fireChannelReadComplete public final void read() { final ChannelConfig config = config(); if (shouldBreakReadReady(config)) { clearReadPending(); return; } final ChannelPipeline pipeline = pipeline(); final ByteBufAllocator allocator = config.getAllocator(); final RecvByteBufAllocator.Handle allocHandle = recvBufAllocHandle(); allocHandle.reset(config); ByteBuf byteBuf = null; boolean close = false; try { do { byteBuf = allocHandle.allocate(allocator); allocHandle.lastBytesRead(doReadBytes(byteBuf)); if (allocHandle.lastBytesRead() &lt;= 0) { // nothing was read. release the buffer. byteBuf.release(); byteBuf = null; close = allocHandle.lastBytesRead() &lt; 0; if (close) { // There is nothing left to read as we received an EOF. readPending = false; } break; } allocHandle.incMessagesRead(1); readPending = false; pipeline.fireChannelRead(byteBuf); byteBuf = null; } while (allocHandle.continueReading()); allocHandle.readComplete(); pipeline.fireChannelReadComplete(); if (close) { closeOnRead(pipeline); } } catch (Throwable t) { handleReadException(pipeline, byteBuf, t, close, allocHandle); } finally { // Check if there is a readPending which was not processed yet. // This could be for two reasons: // * The user called Channel.read() or ChannelHandlerContext.read() in channelRead(...) method // * The user called Channel.read() or ChannelHandlerContext.read() in channelReadComplete(...) method // // See https://github.com/netty/netty/issues/2254 if (!readPending &amp;&amp; !config.isAutoRead()) { removeReadOp(); } } } pipline.fireChannelRead NioServerSocketChannel和NioSocketChannel的实现也是不同的，一个关注对于新accept的NioSocketChannel的初始化；另一个则关注具体的read处理 NioServerSocketChannel #fireChannelRead NSSC中添加了唯一一个handler用于处理child，设置child属性，用child的eventloopGroup异步register ServerBootStrapAcceptor#channelRead ServerBootstrapAcceptor( final Channel channel, EventLoopGroup childGroup, ChannelHandler childHandler, Entry&lt;ChannelOption&lt;?&gt;, Object&gt;[] childOptions, Entry&lt;AttributeKey&lt;?&gt;, Object&gt;[] childAttrs) { this.childGroup = childGroup; this.childHandler = childHandler; this.childOptions = childOptions; this.childAttrs = childAttrs; // Task which is scheduled to re-enable auto-read. // It's important to create this Runnable before we try to submit it as otherwise the URLClassLoader may // not be able to load the class because of the file limit it already reached. // // See https://github.com/netty/netty/issues/1328 enableAutoReadTask = new Runnable() { @Override public void run() { channel.config().setAutoRead(true); } }; } @Override @SuppressWarnings(&quot;unchecked&quot;) public void channelRead(ChannelHandlerContext ctx, Object msg) { final Channel child = (Channel) msg; child.pipeline().addLast(childHandler); setChannelOptions(child, childOptions, logger); setAttributes(child, childAttrs); try { childGroup.register(child).addListener(new ChannelFutureListener() { @Override public void operationComplete(ChannelFuture future) throws Exception { if (!future.isSuccess()) { forceClose(child, future.cause()); } } }); } catch (Throwable t) { forceClose(child, t); } } NioSocketChannel #fireChannelRead NioSocketChannel的read会根据pipline中添加的handler，从head开始以此去执行channelRead，参数的起点就是ByteBuf 自定义handler中的channelRead protected void channelRead0(ChannelHandlerContext ctx, Object msg) throws Exception { ByteBuf buf = (ByteBuf) msg; int capacity = buf.readableBytes(); byte[] bytes = new byte[capacity]; buf.readBytes(bytes); System.out.println(new String(bytes)); ctx.writeAndFlush(Unpooled.copiedBuffer(&quot;ack&quot;.getBytes(StandardCharsets.UTF_8))); } ","link":"https://wjkcoder.github.io/post/netty-nioeventloop-core/"},{"title":"Netty Core ServerBootStrap","content":" Netty Server 构建和启动 Netty 线程模型 Server 属性装配 ServerBootStrap bind() initAndRegister() NioServerSocketChannel实例 ServerBootStrap初始化 ServerSocketChannel注册到EventLoopGroup NioEventLoop异步执行register0方法 执行NIO register方法 doBind0 NioEventLoop异步执行bind 执行AbstractChannel的bind方法 Native bind0 Native listen Netty Server 构建和启动 构建一个基本的Netty Server的核心代码 NioEventLoopGroup core = new NioEventLoopGroup(); NioEventLoopGroup worker = new NioEventLoopGroup(); ServerBootstrap server = new ServerBootstrap().group(core, worker) .channel(NioServerSocketChannel.class) .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() { @Override protected void initChannel(SocketChannel ch) throws Exception { ChannelPipeline pipeline = ch.pipeline(); pipeline.addLast(&quot;handler&quot;, new SimpleChannelInboundHandler&lt;Object&gt;() { @Override protected void channelRead0(ChannelHandlerContext ctx, Object msg) throws Exception { ByteBuf buf = (ByteBuf) msg; int capacity = buf.readableBytes(); byte[] bytes = new byte[capacity]; buf.readBytes(bytes); System.out.println(new String(bytes)); ctx.writeAndFlush(Unpooled.copiedBuffer(&quot;ack&quot;.getBytes(StandardCharsets.UTF_8))); } }); } }); try { ChannelFuture future = server.bind(8001).sync(); future.channel().closeFuture().sync(); } catch (InterruptedException e) { e.printStackTrace(); } finally { core.shutdownGracefully(); worker.shutdownGracefully(); } Netty 线程模型 我个人感觉Netty是没有线程模型这一概念，只有Reactor这一概念； 只有一个线程数为1的group被称之为单线程Reactor模型，是一种很蠢的叫法；不过是只有一个线程或者NioEventLoop做所有的事情；当然用netty的人也不会这么做； boss group线程数为1，worker group为多个，这种被称为多Reactor模型，是第二蠢的叫法；boss group一个线程负责accept，多个worker nioeventloop负责处理读写等事件； boss group为多个，worker group为多个，这种被称为主从多线程Reactor，稍微正常点的叫法； public ServerBootstrap group(EventLoopGroup group) { return group(group, group); } public ServerBootstrap group(EventLoopGroup parentGroup, EventLoopGroup childGroup) { super.group(parentGroup); if (this.childGroup != null) { throw new IllegalStateException(&quot;childGroup set already&quot;); } this.childGroup = ObjectUtil.checkNotNull(childGroup, &quot;childGroup&quot;); return this; } Server 属性装配 channel();实例化Channel工厂 childHandler();处理SocketChannel的核心 Netty实现中有大量的异步操作，future的使用，但都是有迹可循的。 ServerBootStrap bind() bind才是Server启动的真正入口 1.初始化NioServerSocketChannel并注册到时间循环组 2.在1完成后执行绑定 private ChannelFuture doBind(final SocketAddress localAddress) { final ChannelFuture regFuture = initAndRegister();// final Channel channel = regFuture.channel(); if (regFuture.cause() != null) { return regFuture; } if (regFuture.isDone()) { // At this point we know that the registration was complete and successful. ChannelPromise promise = channel.newPromise(); doBind0(regFuture, channel, localAddress, promise); return promise; } else { // Registration future is almost always fulfilled already, but just in case it's not. final PendingRegistrationPromise promise = new PendingRegistrationPromise(channel); regFuture.addListener(new ChannelFutureListener() { @Override public void operationComplete(ChannelFuture future) throws Exception { Throwable cause = future.cause(); if (cause != null) { // Registration on the EventLoop failed so fail the ChannelPromise directly to not cause an // IllegalStateException once we try to access the EventLoop of the Channel. promise.setFailure(cause); } else { // Registration was successful, so set the correct executor to use. // See https://github.com/netty/netty/issues/2586 promise.registered(); doBind0(regFuture, channel, localAddress, promise); } } }); return promise; } } initAndRegister() 用于初始化NioServerSocketChannel final ChannelFuture initAndRegister() { Channel channel = null; try { channel = channelFactory.newChannel();//工厂创建Channel init(channel);//调用Channel的init方法初始化 } ChannelFuture regFuture = config().group().register(channel);//将channel注册到时间循环组中 if (regFuture.cause() != null) { if (channel.isRegistered()) { channel.close(); } else { channel.unsafe().closeForcibly(); } } return regFuture; } NioServerSocketChannel 实例化 反射创建 public T newChannel() { try { return constructor.newInstance(); } catch (Throwable t) { throw new ChannelException(&quot;Unable to create Channel from class &quot; + constructor.getDeclaringClass(), t); } } ServerBootStrap初始化 1.设置Channel的属性 2.给channel的pipline添加了一个ChannelHandler，其中实现了initChannel方法，当ServerSocketChannel就绪时，在其pipline上添加一个特定的Acceptor void init(Channel channel) { setChannelOptions(channel, newOptionsArray(), logger); setAttributes(channel, attrs0().entrySet().toArray(EMPTY_ATTRIBUTE_ARRAY)); ChannelPipeline p = channel.pipeline(); final EventLoopGroup currentChildGroup = childGroup; final ChannelHandler currentChildHandler = childHandler; final Entry&lt;ChannelOption&lt;?&gt;, Object&gt;[] currentChildOptions; synchronized (childOptions) { currentChildOptions = childOptions.entrySet().toArray(EMPTY_OPTION_ARRAY); } final Entry&lt;AttributeKey&lt;?&gt;, Object&gt;[] currentChildAttrs = childAttrs.entrySet().toArray(EMPTY_ATTRIBUTE_ARRAY); p.addLast(new ChannelInitializer&lt;Channel&gt;() { @Override public void initChannel(final Channel ch) { final ChannelPipeline pipeline = ch.pipeline(); ChannelHandler handler = config.handler(); if (handler != null) { pipeline.addLast(handler); } ch.eventLoop().execute(new Runnable() { @Override public void run() { pipeline.addLast(new ServerBootstrapAcceptor( ch, currentChildGroup, currentChildHandler, currentChildOptions, currentChildAttrs)); } }); } }); } ServerSocketChannel注册到EventLoopGroup 1.从group的chooser中选一个eventloop 2.channel绑定到eventloop 3.eventloop去执行register0 ChannelFuture regFuture = config().group().register(channel); public ChannelFuture register(Channel channel) { return next().register(channel); } public ChannelFuture register(Channel channel) { return register(new DefaultChannelPromise(channel, this)); } public ChannelFuture register(final ChannelPromise promise) { ObjectUtil.checkNotNull(promise, &quot;promise&quot;); promise.channel().unsafe().register(this, promise); return promise; } public final void register(EventLoop eventLoop, final ChannelPromise promise) { ObjectUtil.checkNotNull(eventLoop, &quot;eventLoop&quot;); if (isRegistered()) { promise.setFailure(new IllegalStateException(&quot;registered to an event loop already&quot;)); return; } if (!isCompatible(eventLoop)) { promise.setFailure( new IllegalStateException(&quot;incompatible event loop type: &quot; + eventLoop.getClass().getName())); return; } AbstractChannel.this.eventLoop = eventLoop; if (eventLoop.inEventLoop()) { register0(promise); } else { try { eventLoop.execute(new Runnable() { @Override public void run() { register0(promise); } }); } catch (Throwable t) { logger.warn( &quot;Force-closing a channel whose registration task was not accepted by an event loop: {}&quot;, AbstractChannel.this, t); closeForcibly(); closeFuture.setClosed(); safeSetFailure(promise, t); } } } NioEventLoop异步执行register0方法 1.执行doRegister 2.执行pipline的handlerAdded 3.返回promise true 4.执行fireChannelRegister 5.执行fireChannelActive private void register0(ChannelPromise promise) { try { doRegister(); neverRegistered = false; registered = true; pipeline.invokeHandlerAddedIfNeeded(); safeSetSuccess(promise); pipeline.fireChannelRegistered(); // Only fire a channelActive if the channel has never been registered. This prevents firing // multiple channel actives if the channel is deregistered and re-registered. if (isActive()) { if (firstRegistration) { pipeline.fireChannelActive(); } else if (config().isAutoRead()) { // This channel was registered before and autoRead() is set. This means we need to begin read // again so that we process inbound data. // // See https://github.com/netty/netty/issues/4805 beginRead(); } } } } 执行NIO register方法 本质上上执行了nio的ServerSocketChannel.register(selector,ops,att) 注册兴趣ops，把自己放到att里 protected void doRegister() throws Exception { boolean selected = false; for (;;) { try { selectionKey = javaChannel().register(eventLoop().unwrappedSelector(), 0, this); return; } } public final SelectionKey register(Selector sel, int ops, Object att) throws ClosedChannelException { synchronized (regLock) { SelectionKey k = findKey(sel); if (k != null) { k.interestOps(ops); k.attach(att); } if (k == null) { // New registration synchronized (keyLock) { if (!isOpen()) throw new ClosedChannelException(); k = ((AbstractSelector)sel).register(this, ops, att); addKey(k); } } return k; } } protected final SelectionKey register(AbstractSelectableChannel var1, int var2, Object var3) { if (!(var1 instanceof SelChImpl)) { throw new IllegalSelectorException(); } else { SelectionKeyImpl var4 = new SelectionKeyImpl((SelChImpl)var1, this); var4.attach(var3); synchronized(this.publicKeys) { this.implRegister(var4); } var4.interestOps(var2); return var4; } } protected void implRegister(SelectionKeyImpl var1) { if (this.closed) { throw new ClosedSelectorException(); } else { int var2 = IOUtil.fdVal(var1.channel.getFD()); this.fdMap.put(var2, new MapEntry(var1)); ++this.totalChannels; this.keys.add(var1); } } 以上的初始化和注册操作完成后，会在listener中加一个操作完成的响应时间，执行doBind0 doBind0 NioEventLoop异步执行bind channel的eventloop去执行bind操作，此时channel和eventloop已经注册完毕 private static void doBind0( final ChannelFuture regFuture, final Channel channel, final SocketAddress localAddress, final ChannelPromise promise) { // This method is invoked before channelRegistered() is triggered. Give user handlers a chance to set up // the pipeline in its channelRegistered() implementation. channel.eventLoop().execute(new Runnable() { @Override public void run() { if (regFuture.isSuccess()) { channel.bind(localAddress, promise).addListener(ChannelFutureListener.CLOSE_ON_FAILURE); } else { promise.setFailure(regFuture.cause()); } } }); } public ChannelFuture bind(SocketAddress localAddress, ChannelPromise promise) { return pipeline.bind(localAddress, promise); } public final ChannelFuture bind(SocketAddress localAddress, ChannelPromise promise) { return tail.bind(localAddress, promise); } public ChannelFuture bind(final SocketAddress localAddress, final ChannelPromise promise) { ObjectUtil.checkNotNull(localAddress, &quot;localAddress&quot;); if (isNotValidPromise(promise, false)) { // cancelled return promise; } final AbstractChannelHandlerContext next = findContextOutbound(MASK_BIND); EventExecutor executor = next.executor(); if (executor.inEventLoop()) { next.invokeBind(localAddress, promise); } else { safeExecute(executor, new Runnable() { @Override public void run() { next.invokeBind(localAddress, promise); } }, promise, null, false); } return promise; } public void bind( ChannelHandlerContext ctx, SocketAddress localAddress, ChannelPromise promise) { unsafe.bind(localAddress, promise); } 执行AbstractChannel的bind方法 1.执行doBind，本质上是执行nio的ServerSocketChannel.bind() 2.执行fireChannelActive 3.promise设置true public final void bind(final SocketAddress localAddress, final ChannelPromise promise) { assertEventLoop(); boolean wasActive = isActive(); try { doBind(localAddress); } catch (Throwable t) { safeSetFailure(promise, t); closeIfClosed(); return; } if (!wasActive &amp;&amp; isActive()) { invokeLater(new Runnable() { @Override public void run() { pipeline.fireChannelActive(); } }); } safeSetSuccess(promise); } public ServerSocketChannel bind(SocketAddress var1, int var2) throws IOException { InetSocketAddress var4 = var1 == null ? new InetSocketAddress(0) : Net.checkAddress(var1); SecurityManager var5 = System.getSecurityManager(); if (var5 != null) { var5.checkListen(var4.getPort()); } NetHooks.beforeTcpBind(this.fd, var4.getAddress(), var4.getPort()); Net.bind(this.fd, var4.getAddress(), var4.getPort()); Net.listen(this.fd, var2 &lt; 1 ? 50 : var2); synchronized(this.stateLock) { this.localAddress = Net.localAddress(this.fd); } } Native bind0 rv = bind(fd, him, len); 最终调用socket.h#bind int bind(int, const struct sockaddr *, socklen_t) __DARWIN_ALIAS(bind); Native listen 最终调用socket.h#listen int listen(int, int) __DARWIN_ALIAS(listen); ","link":"https://wjkcoder.github.io/post/netty-serverbootstrap/"},{"title":"Netty Core NioEventLoopGroup","content":" NioEventLoopGroup 实例化 NioEventLoop实例化 Selector 实例化 KQueueSelectorProvider 实例化 KqueueSelectorImpl 实例化 KQueueArrayWrapper 实例化 1.访问直接内存 2.创建系统kqueue KQueueArrayWrapper注册事件 JNI中的register kevent系统函数 NioEventLoopGroup 实例化 本质上是一个线程数组，除此之外加入了事件驱动的机制。 传入线程个数时，会生成指定个数的数组；无参数会根据当前的CPU核心数*2创建； NioEventLoopGroup core = new NioEventLoopGroup(); public NioEventLoopGroup(int nThreads, Executor executor, final SelectorProvider selectorProvider, final SelectStrategyFactory selectStrategyFactory) { super(nThreads, executor, selectorProvider, selectStrategyFactory, RejectedExecutionHandlers.reject()); } protected MultithreadEventLoopGroup(int nThreads, Executor executor, Object... args) { super(nThreads == 0 ? DEFAULT_EVENT_LOOP_THREADS : nThreads, executor, args); } private static final int DEFAULT_EVENT_LOOP_THREADS; static { DEFAULT_EVENT_LOOP_THREADS = Math.max(1, SystemPropertyUtil.getInt( &quot;io.netty.eventLoopThreads&quot;, NettyRuntime.availableProcessors() * 2)); if (logger.isDebugEnabled()) { logger.debug(&quot;-Dio.netty.eventLoopThreads: {}&quot;, DEFAULT_EVENT_LOOP_THREADS); } } 遍历数组，初始化每一个child，即eventloop，这里是调用NioEventLoopGroup的newChild方法； protected MultithreadEventExecutorGroup(int nThreads, Executor executor, EventExecutorChooserFactory chooserFactory, Object... args) { //核心代码 children = new EventExecutor[nThreads]; for (int i = 0; i &lt; nThreads; i ++) { boolean success = false; try { children[i] = newChild(executor, args); success = true; } catch (Exception e) { // TODO: Think about if this is a good exception type throw new IllegalStateException(&quot;failed to create a child event loop&quot;, e); } finally { //一个失败，全部回收 if (!success) { for (int j = 0; j &lt; i; j ++) { children[j].shutdownGracefully(); } for (int j = 0; j &lt; i; j ++) { EventExecutor e = children[j]; try { while (!e.isTerminated()) { e.awaitTermination(Integer.MAX_VALUE, TimeUnit.SECONDS); } } catch (InterruptedException interrupted) { // Let the caller handle the interruption. Thread.currentThread().interrupt(); break; } } } } } chooser = chooserFactory.newChooser(children);//创建一个线程选择调度器用于分配线程 } NioEventLoop实例化 通过spi的方式拿到selectorProvider，macos下是KQueueSelectorProvider实现； 在openSelector中创建selector实例； NioEventLoop(NioEventLoopGroup parent, Executor executor, SelectorProvider selectorProvider, SelectStrategy strategy, RejectedExecutionHandler rejectedExecutionHandler, EventLoopTaskQueueFactory queueFactory) { super(parent, executor, false, newTaskQueue(queueFactory), newTaskQueue(queueFactory), rejectedExecutionHandler); this.provider = ObjectUtil.checkNotNull(selectorProvider, &quot;selectorProvider&quot;); this.selectStrategy = ObjectUtil.checkNotNull(strategy, &quot;selectStrategy&quot;); final SelectorTuple selectorTuple = openSelector(); this.selector = selectorTuple.selector;//这个是包装了一层的selector this.unwrappedSelector = selectorTuple.unwrappedSelector;//这个是原生的selector } Selector 实例化 private SelectorTuple openSelector() { final Selector unwrappedSelector; try { unwrappedSelector = provider.openSelector(); } catch (IOException e) { throw new ChannelException(&quot;failed to open a new selector&quot;, e); } } SelectedSelectionKeySetSelector(Selector delegate, SelectedSelectionKeySet selectionKeys) { this.delegate = delegate; this.selectionKeys = selectionKeys; } KQueueSelectorProvider 实例化 public class KQueueSelectorProvider extends SelectorProviderImpl { public KQueueSelectorProvider() { } public AbstractSelector openSelector() throws IOException { return new KQueueSelectorImpl(this); } } KqueueSelectorImpl 实例化 KQueueSelectorImpl(SelectorProvider var1) { super(var1); long var2 = IOUtil.makePipe(false); this.fd0 = (int)(var2 &gt;&gt;&gt; 32); this.fd1 = (int)var2; try { this.kqueueWrapper = new KQueueArrayWrapper(); this.kqueueWrapper.initInterrupt(this.fd0, this.fd1); this.fdMap = new HashMap(); this.totalChannels = 1; } } 创建KQueueArrayWrapper实例化之后，注册兴趣事件； KQueueArrayWrapper 实例化 KAW是与系统调用联结的核心，通过jni调用kevent.h定义的事件函数来完成事件驱动； KQueueArrayWrapper() { int var1 = SIZEOF_KEVENT * 128;//计算总的内存长度 this.keventArray = new AllocatedNativeObject(var1, true);//分配内存返回首字节的地址 this.keventArrayAddress = this.keventArray.address(); this.kq = this.init();//调用kqueue创建fd } 核心点有几个： 1.访问直接内存 方便之后通过偏移量直接访问内存数据(fd,filter) SIZEOF_KEVENT=32;//一个kevent struct的大小 FD_OFFSE=0; FILTER_OFFSET=8; //初始化静态块 static { IOUtil.load(); initStructSizes(); String var0 = (String)AccessController.doPrivileged(new GetPropertyAction(&quot;sun.arch.data.model&quot;)); is64bit = var0.equals(&quot;64&quot;); } JNI初始化值 field = (*env)-&gt;GetStaticFieldID(env, clazz, &quot;SIZEOF_KEVENT&quot;, &quot;S&quot;); CHECK_ERROR_AND_EXCEPTION(field); (*env)-&gt;SetStaticShortField(env, clazz, field, (short) sizeof(struct kevent)); CHECK_EXCEPTION(); field = (*env)-&gt;GetStaticFieldID(env, clazz, &quot;FD_OFFSET&quot;, &quot;S&quot;); CHECK_ERROR_AND_EXCEPTION(field); (*env)-&gt;SetStaticShortField(env, clazz, field, (short) offsetof(struct kevent, ident)); CHECK_EXCEPTION(); field = (*env)-&gt;GetStaticFieldID(env, clazz, &quot;FILTER_OFFSET&quot;, &quot;S&quot;); CHECK_ERROR_AND_EXCEPTION(field); (*env)-&gt;SetStaticShortField(env, clazz, field, (short) offsetof(struct kevent, filter)); 系统调用中kevent数据结构 struct kevent { uintptr_t ident; /* identifier for this event */ int16_t filter; /* filter for event */ uint16_t flags; /* general flags */ uint32_t fflags; /* filter-specific flags */ intptr_t data; /* filter-specific data */ void *udata; /* opaque user data identifier */ }; 2.创建系统kqueue this.kq = this.init(); private native int init(); JNIEXPORT jint JNICALL Java_sun_nio_ch_KQueueArrayWrapper_init(JNIEnv *env, jobject this) { int kq = kqueue(); if (kq &lt; 0) { JNU_ThrowIOExceptionWithLastError(env, &quot;KQueueArrayWrapper: kqueue() failed&quot;); } return kq; } KQueueArrayWrapper注册事件 jni调用kevent()编辑/修改兴趣事件 这里的实际效果是添加一个read，删除一个write this.kqueueWrapper.initInterrupt(this.fd0, this.fd1); void initInterrupt(int var1, int var2) { this.outgoingInterruptFD = var2; this.incomingInterruptFD = var1; this.register0(this.kq, var1, 1, 0); } private native void register0(int var1, int var2, int var3, int var4); JNI中的register JNIEXPORT void JNICALL Java_sun_nio_ch_KQueueArrayWrapper_register0(JNIEnv *env, jobject this, jint kq, jint fd, jint r, jint w) { struct kevent changes[2]; struct kevent errors[2]; struct timespec dontBlock = {0, 0}; // if (r) then { register for read } else { unregister for read } // if (w) then { register for write } else { unregister for write } // Ignore errors - they're probably complaints about deleting non- // added filters - but provide an error array anyway because // kqueue behaves erratically if some of its registrations fail. EV_SET(&amp;changes[0], fd, EVFILT_READ, r ? EV_ADD : EV_DELETE, 0, 0, 0); EV_SET(&amp;changes[1], fd, EVFILT_WRITE, w ? EV_ADD : EV_DELETE, 0, 0, 0); kevent(kq, changes, 2, errors, 2, &amp;dontBlock); } kevent系统函数 展开解释下kevent系统函数 kq是kqueue的identifier changelist表示对fd的修改列表，nchanges表示数量； eventlist表示系统产生的事件列表，用于存放系统产生的事件，nevents表示数量； timeout表示最大等待时长； int kevent(int kq, const struct kevent *changelist, int nchanges, struct kevent *eventlist, int nevents, const struct timespec *timeout); 到此事件循环组的初始化的核心过程已经完成； ","link":"https://wjkcoder.github.io/post/netty-nioeventloopgroup/"}]}